# Билеты к зачёту по курсу ИС — развернутые ответы (Markdown)

> Основа: методичка `ZAChET_PO_IS.docx`, дополнения — открытые источники там, где в методичке информация сильно краткая.

---

## Билет 1. Естественный интеллект, ИИ и МО. Терминология. Постановка задачи МО

### Понятия
- **Естественный интеллект (ЕИ)** — интеллект человека/живых систем: восприятие, обучение, обобщение, рассуждение, целеполагание.
- **Искусственный интеллект (ИИ)** — инженерные методы и системы, решающие задачи, требующие “интеллектуального” поведения (распознавание, планирование, анализ, язык).
- **Машинное обучение (МО)** — раздел ИИ, где модель **обучается по данным**, извлекая закономерности, вместо задания всех правил вручную.

### Термины
- **Признаки**: \(x\in X\), **ответ/метка**: \(y\in Y\).
- **Выборка**: \(D=\{(x_i,y_i)\}_{i=1}^n\).
- **Модель**: \(f(x;\theta)\), параметры \(\theta\), **гиперпараметры** (learning rate, batch size, архитектура, \(\lambda\)).
- **Функция потерь**: \(L(y,\hat y)\), **метрика**: accuracy/F1/ROC-AUC/MSE и т.д.

### Постановка задачи МО
Цель — найти параметры \(\theta\), минимизирующие ошибку на данных (и обобщающие на новые данные):
\[
\theta^\*=\arg\min_\theta \frac1n\sum_{i=1}^n L\big(y_i,f(x_i;\theta)\big)+\lambda R(\theta)
\]
где \(R(\theta)\) — регуляризация (штраф за сложность).

### Что важно проговорить на зачёте
- Минимизируем **эмпирический риск** (на выборке), а интересует **истинный риск** (на распределении) → нужна валидация/тест.
- Риски: переобучение, смещение данных, OOD.

---

## Билет 2. Виды МО и примеры задач

### 1) Обучение с учителем (supervised)
Есть пары \((x,y)\).
- **Классификация**: \(y\in\{1..K\}\) (спам/не спам, класс изображения).
- **Регрессия**: \(y\in\mathbb R\) (цена, температура, нагрузка).

### 2) Обучение без учителя (unsupervised)
Есть только \(x\), ищем структуру:
- кластеризация (k-means, GMM),
- понижение размерности (PCA),
- поиск аномалий (плотность, reconstruction error).

### 3) Обучение с подкреплением (RL)
Агент выбирает действия и получает награду:
- игры, робототехника, управление.

### 4) Self-supervised / semi-supervised
- **Self-supervised**: псевдометки извлекаются из данных (предсказать пропуск/следующий токен).
- **Semi-supervised**: мало размеченных + много неразмеченных данных.

---

## Билет 3. Байес. Термины. Байесовская бинарная классификация (1 признак)

### Формула Байеса
\[
P(y|x)=\frac{P(x|y)P(y)}{P(x)},\quad P(x)=\sum_k P(x|y=k)P(y=k)
\]
- \(P(y)\) — априорная вероятность класса,
- \(P(x|y)\) — правдоподобие,
- \(P(y|x)\) — апостериорная вероятность.

### Байесовское решающее правило (binary)
\[
\hat y=\arg\max_{y\in\{0,1\}} P(y|x)\equiv \arg\max_y P(x|y)P(y)
\]

### Однопризнаковый случай (1D)
Если \(x|y=k\sim \mathcal N(\mu_k,\sigma_k^2)\), то сравнивают:
\[
\ln P(x|1)+\ln P(1)\;\;\text{vs}\;\;\ln P(x|0)+\ln P(0)
\]
- если \(\sigma_0=\sigma_1\) → граница линейная (порог),
- если \(\sigma_0\neq\sigma_1\) → граница квадратичная.

---

## Билет 4. Регуляризация на полиномиальной регрессии. Регуляризация в DNN

### Переобучение на полиномах
Полином высокой степени:
\[
\hat y = \sum_{j=0}^{m} w_j x^j
\]
может “подогнать шум”, давая низкую ошибку на train и плохую на test.

### L2-регуляризация (ridge / weight decay)
\[
L=\sum_i (y_i-\hat y_i)^2+\lambda \sum_j w_j^2
\]
Снижает величину коэффициентов → гладкость, меньшая вариативность.

### L1-регуляризация (lasso)
\[
L=\sum_i (y_i-\hat y_i)^2+\lambda \sum_j |w_j|
\]
Способствует разреженности (часть \(w_j=0\)).

### В DNN
- weight decay (L2),
- dropout,
- data augmentation,
- early stopping,
- (часто) BN/LN как стабилизатор.

---

## Билет 5. Функция ошибки и градиентный спуск. Adagrad / RMSProp / Adam

### Градиентный спуск
\[
\theta_{t+1}=\theta_t-\eta \nabla_\theta L(\theta_t)
\]
На практике — SGD / mini-batch.

### Adagrad
\[
G_t=G_{t-1}+g_t^2,\quad \theta_{t+1}=\theta_t-\eta\frac{g_t}{\sqrt{G_t}+\epsilon}
\]
Шаг уменьшается со временем (может “затухнуть”).

### RMSProp
\[
v_t=\rho v_{t-1}+(1-\rho)g_t^2,\quad \theta_{t+1}=\theta_t-\eta\frac{g_t}{\sqrt{v_t}+\epsilon}
\]
Скользящее среднее квадратов градиента — нет сильного затухания.

### Adam
\[
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t,\quad v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2
\]
\[
\hat m_t=\frac{m_t}{1-\beta_1^t},\quad \hat v_t=\frac{v_t}{1-\beta_2^t},\quad
\theta_{t+1}=\theta_t-\eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}
\]

---

## Билет 6. Ошибка регрессии и модель шума

### Модель регрессии
\[
y=f(x;\theta)+\varepsilon
\]

Если \(\varepsilon\sim \mathcal N(0,\sigma^2)\), то максимизация правдоподобия эквивалентна минимизации MSE:
\[
\text{MSE}=\frac1n\sum_i (y_i-\hat y_i)^2
\]

### Другие шумы
- Laplace → MAE,
- выбросы → Huber-loss (устойчивость к выбросам).

---

## Билет 7. Ошибка классификации. KL и перекрёстная энтропия

### Cross-entropy
\[
H(p,q)=-\sum_k p_k\log q_k
\]
Для one-hot \(p\): \(H=-\log q_y\).

### KL-дивергенция
\[
D_{KL}(p\|q)=\sum_k p_k\log\frac{p_k}{q_k}
\]
Связь:
\[
H(p,q)=H(p)+D_{KL}(p\|q)
\]
Минимизация \(H(p,q)\) ⇔ минимизация \(D_{KL}\), т.к. \(H(p)\) — константа.

---

## Билет 8. ИНС. MLP. Backprop. Недостатки

### Архитектура MLP
\[
z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)},\quad a^{(l)}=\phi(z^{(l)})
\]

### Обучение
Backpropagation (цепное правило) + оптимизация (SGD/Adam).

### Недостатки
- исчезающий/взрывающийся градиент,
- переобучение,
- много параметров (особенно на изображениях),
- слабая “структурность” по сравнению с CNN.

---

## Билет 9. Backprop формулы. Online/Batch. Методы 2-го порядка

### Базовые формулы
\[
\delta^l=\frac{\partial L}{\partial z^l}
\]
Выход:
\[
\delta^L=\nabla_{a^L}L\odot \phi'(z^L)
\]
Скрытый:
\[
\delta^l=((W^{l+1})^T\delta^{l+1})\odot \phi'(z^l)
\]
Градиенты:
\[
\frac{\partial L}{\partial W^l}=\delta^l(a^{l-1})^T,\quad \frac{\partial L}{\partial b^l}=\delta^l
\]

### Online/Batch/Mini-batch
- online: по 1 примеру,
- batch: по всему датасету,
- mini-batch: компромисс и стандарт.

### Методы 2-го порядка
Ньютона, (L-)BFGS, Gauss-Newton, Hessian-free, Левенберг–Марквардт — учитывают кривизну, но тяжело масштабируются для больших DNN.

---

## Билет 10. Функции активации. Зачем разные. Граф вычислений

### Зачем активации
Без нелинейностей любая композиция линейных слоёв остаётся линейной.

### Популярные
- sigmoid/tanh — насыщаются,
- ReLU — простая и эффективная,
- LeakyReLU/ELU/GELU — улучшают градиенты/гладкость.

### Граф вычислений
Функция разбивается на элементарные операции (DAG), backprop — дифференцирование по цепному правилу вдоль графа.

---

## Билет 11. BatchNorm. Internal covariate shift

### Проблема
Распределение активаций слоёв “плывёт” в ходе обучения → сложнее оптимизация.

### BatchNorm (идея)
\[
\hat x=\frac{x-\mu_B}{\sqrt{\sigma_B^2+\epsilon}},\quad y=\gamma\hat x+\beta
\]
\(\gamma,\beta\) — обучаемые параметры, возвращают выразительность.

---

## Билет 12. CNN: conv/pooling/softmax

### Conv
- локальные рецептивные поля,
- разделение параметров (shared weights),
- параметры: kernel size, stride, padding, filters.

### Pooling
- downsampling,
- устойчивость к небольшим сдвигам,
- тип: max/avg.

### Softmax
\[
p_k=\frac{e^{z_k}}{\sum_j e^{z_j}}
\]
Интерпретация как вероятности + cross-entropy.

---

## Билет 13. Борьба с исчезающим градиентом

### Причины
- sigmoid/tanh → малые производные,
- глубокая композиция производных.

### Решения
- ReLU-family,
- BatchNorm/LayerNorm,
- хорошая инициализация (Xavier/He),
- residual/skip connections,
- gradient clipping (для взрыва),
- LSTM/GRU для последовательностей,
- предобучение входных слоёв.

---

## Билет 14. Метрики классификации и регрессии. ROC

### Классификация
- Accuracy, Precision, Recall, F1,
- ROC (TPR vs FPR), AUC,
- PR-кривая (важна при дисбалансе).

### Регрессия
- MAE, MSE, RMSE,
- \(R^2\).

### ROC-определения
\[
TPR=\frac{TP}{TP+FN},\quad FPR=\frac{FP}{FP+TN}
\]

---

## Билет 15. Устойчивость вне обучающей области. One-class. EDCA (идея)

### Проблема OOD
Модель может быть уверенной на данных вне распределения обучающих примеров.

### One-class классификация
Обучаем “норму” и проверяем отклонение:
- One-Class SVM,
- AE по ошибке реконструкции,
- плотностные модели,
- порог по score.

### Идея EDCA (по методичке)
Сравнение области “нормы” (по данным) и области, которую модель относит к целевому классу:
- **Excess** (лишнее),
- **Deficit** (недостающее),
- **Coating** (покрытие),
- **Approx** (аппроксимация границ).

---

## Билет 16. Автокодировщики, латентные признаки. Виды. Применение

### Архитектура
Encoder \(x\to z\), Decoder \(z\to \hat x\), обучение по реконструкции:
\[
L=\|x-\hat x\|
\]

### Виды
- denoising AE,
- sparse AE,
- convolutional AE,
- VAE.

### Применения
- предобучение/инициализация,
- transfer learning (encoder как feature extractor),
- аномалии: рост reconstruction error.

---

## Билет 17. Сложность обучения DNN. SIMD и GPGPU

### Что делает обучение дорогим
- число параметров и операций,
- backprop (градиенты),
- память для активаций/градиентов.

### Ускорение
- SIMD (векторизация на CPU),
- GPU (GPGPU),
- оптимизированные библиотеки (BLAS/cuDNN),
- mixed precision, распределённое обучение.

---

## Билет 18. Распознавание изображений. CNN. Предобработка. CIFAR-10, ImageNet

### Почему CNN
- локальные признаки,
- иерархия представлений,
- меньше параметров, чем MLP.

### Предобработка
- нормализация,
- resize/crop,
- аугментации (flip/rotate/color jitter),
- балансировка классов.

### Датасеты
- CIFAR-10: 10 классов, 32×32,
- ImageNet: большой benchmark классификации (исторически важный).

---

## Билет 19. Эволюция CNN: LeNet → AlexNet → ResNet → Inception → Inception-v2; факторизация; skip

### LeNet-5
Ранняя CNN (цифры), conv + pooling + FC.

### AlexNet
ReLU, dropout, data augmentation, GPU-обучение, прорыв на ImageNet.

### Inception/GoogLeNet
Параллельные ветви разных масштабов + 1×1 bottleneck.

### Inception-v2
Факторизация \(N\times N\to 1\times N + N\times 1\) для уменьшения вычислений.

### ResNet
Остаточные связи:
\[
y=x+F(x)
\]
облегчают прохождение градиента и позволяют очень глубокие сети.

---

## Билет 20. Последовательности и RNN: задачи, обучение, проблемы

### Типы задач
- many-to-one, one-to-many, many-to-many,
- tagging (метка на каждом шаге).

### RNN
\[
h_t=\phi(W_x x_t + W_h h_{t-1}+b)
\]

### Обучение и проблемы
- BPTT,
- исчезающий/взрывающийся градиент,
- сложность с дальними зависимостями,
- непараллельность во времени.

---

## Билет 21. LSTM: устройство, плюсы/минусы

### Идея
Есть память \(c_t\) и гейты, которые управляют потоком информации.

### Основные формулы
\[
c_t=f_t\odot c_{t-1}+i_t\odot \tilde c_t,\quad
h_t=o_t\odot \tanh(c_t)
\]
где \(f_t,i_t,o_t\) — forget/input/output gates.

### Плюсы
- хорошо держит долгие зависимости,
- стабильнее градиенты.

### Минусы
- больше параметров и вычислений,
- сложнее настройка и интерпретация,
- риск переобучения.

---

## Билет 22. Развитие LSTM. GRU

### GRU
Упрощение LSTM:
- update gate, reset gate,
- меньше параметров,
- часто быстрее обучение при сопоставимом качестве.

---

## Билет 23. Генеративные и состязательные модели. DeepFake на автоэнкодерах

### Генеративные модели
Учат распределение данных \(p(x)\) или \(p(x|z)\).

### GAN
Генератор \(G\) и дискриминатор \(D\) обучаются в минимакс-игре:
\[
\min_G\max_D \mathbb E[\log D(x)]+\mathbb E[\log(1-D(G(z)))]
\]

### DeepFake через AE
Общий encoder + разные decoder для лиц; меняем decoder — переносим “идентичность”.

---

## Билет 24. Transformer: внимание в encoder

### Attention
\[
Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt d}\right)V
\]

### Encoder
- multi-head self-attention,
- FFN,
- residual + layer norm,
- positional encoding.

### Почему лучше RNN
- параллельная обработка токенов,
- лучше учит дальние зависимости.

---

## Билет 25. Transformer: decoder и генерация

### Decoder
- masked self-attention (не видит будущие токены),
- cross-attention к encoder (в seq2seq),
- FFN, residual + norm.

### Генерация
Авто-регрессия: предсказываем следующий токен, добавляем, повторяем до EOS.

---

## Билет 26. AE + взаимные корреляции для аномалий динамических систем

### Идея
- AE обучают на “норме”: аномалия → рост reconstruction error.
- Корреляции между каналами в норме стабильны; аномалия нарушает структуру взаимосвязей.

### Практика
Скор:
- \(RE(t)=\|x(t)-\hat x(t)\|\),
- признаки корреляций/лагов,
- порог по квантилю нормального режима.

---

## Билет 27. Нейросети для обфускации вычислений

### Суть
Использование сетей как “непрозрачных” преобразований:
- скрыть структуру алгоритма,
- усложнить анализ представлений,
- применять watermarking/защиту модели.

> Важно: это не криптографическая гарантия, а усложнение анализа.

---

## Билет 28. Доверенный ИИ: определение, проблемы, подходы

### Определение (смысл)
Доверенный ИИ — система, которая:
- корректна и надёжна,
- устойчива,
- объяснима,
- безопасна и защищена,
- соответствует требованиям (аудит/регуляторика).

### Проблемы
- bias,
- OOD,
- adversarial,
- утечки данных,
- “чёрный ящик”.

### Подходы
- управление рисками и мониторинг,
- XAI,
- robust training,
- контроль качества/происхождения данных,
- red teaming и безопасность ML.

---

## Билет 29. Атаки на ИИ: adversarial, backdoor, poisoning, supply chain

### Adversarial
Малые возмущения входа → неверный ответ.

### Poisoning
Отравление обучающих данных/обновлений.

### Backdoor
Триггер → скрытое поведение модели при нормальном качестве на обычных данных.

### Supply chain
Компрометация датасета, pretrained весов, библиотек, пайплайна CI/CD.

---

## Билет 30. Устойчивые к adversarial классификаторы: метод фонового класса

### Идея
Добавить класс **background/other**:
- сомнительные/OOD/атакующие примеры уходят в “фон” вместо ошибочного целевого класса.

### Обучение
- \(K\) основных классов + \(1\) фоновый,
- фон заполняют нерелевантными данными/искажениями/частью adversarial.

### Плюсы/минусы
- + меньше уверенных ошибок на атаках/OOD,
- − возможен рост “отказов” на сложных нормальных примерах.

---